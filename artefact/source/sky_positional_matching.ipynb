{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"sky_positional_matching.ipynb\n",
    "James Gardner 2019\n",
    "\n",
    "generates primitive feature vector catalogue for entire sky covered by\n",
    "both surveys, elements are positional matches of one source from TGSS\n",
    "and one source form NVSS with the SEPARATION_DIST of 2', note that this\n",
    "is less than that of 10' for the smaller patch, this is done to improve runtime\n",
    "\n",
    "standard execution, all cells top to bottom:\n",
    "requires TGSSADR1_7sigma_catalog.tsv and CATALOG.FIT be present in cwd\n",
    "produces sky_matches.csv, sky_catalogue.csv, hist_angle.pdf, and hist_alpha.pdf\n",
    "\n",
    "alternative execution:\n",
    "have the provided sky_matches.csv (found in source folder) present in cwd\n",
    "comment out/delete the third (3rd) cell below that calls generate_matches (search for 'delete this')\n",
    "execute all (remaining) cells from top to bottom\n",
    "requires existing sky_matches.csv and well as\n",
    "TGSSADR1_7sigma_catalog.tsv and CATALOG.FIT be present in cwd\n",
    "modified sky_positional_matching.ipynb only saves\n",
    "sky_catalogue.csv, hist_angle.pdf, and hist_alpha.pdf\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# separation limit, 2'\n",
    "SEPARATION_DIST = 2*1/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matches():\n",
    "    \"\"\"generates sky_matches.csv, containing all pairs from tgss, nvss\n",
    "    s.t. a basic estimate for their separation is closer than SEPARATION_DIST\n",
    "    requires TGSSADR1_7sigma_catalog.tsv and CATALOG.FIT be present in cwd\n",
    "    \"\"\"\n",
    "    # load ra,dec from tgss \n",
    "    tgss = np.genfromtxt(\n",
    "        fname=\"TGSSADR1_7sigma_catalog.tsv\",\n",
    "        delimiter='\\t',\n",
    "        skip_header=1,\n",
    "        usecols=(1,3))\n",
    "    tgss = tgss[tgss[:,1].argsort()]    \n",
    "\n",
    "    # load ra,dec from nvss\n",
    "    with fits.open(\"CATALOG.FIT\") as hdulist:\n",
    "        data = hdulist[1].data\n",
    "        nvss = np.column_stack((data['RA(2000)'],data['DEC(2000)']))\n",
    "        # uncomment lines to examine NVSS catalogue\n",
    "        #hdulist.info()\n",
    "        #hdr = hdulist[0].header\n",
    "        #print(repr(hdr))\n",
    "        #cols = hdulist[1].columns\n",
    "        #cols.info()    \n",
    "    nvss = nvss[nvss[:,1].argsort()]\n",
    "\n",
    "    print('survey lengths',np.shape(tgss),np.shape(nvss))\n",
    "\n",
    "    # constructing 0.1 deg lookup bins of nvss to speed up\n",
    "    # for loop of the cross of the two surveys\n",
    "    nvss_dec_min = round(nvss[:,1].min(),1)\n",
    "    nvss_dec_max = round(nvss[:,1].max(),1)\n",
    "\n",
    "    # chunkis is index of where bin starts\n",
    "    bin_size = 0.1\n",
    "    mark = nvss_dec_min - bin_size\n",
    "    chunkis = []\n",
    "    count = 0\n",
    "    # finding the first element in each bin\n",
    "    for i,dec in enumerate(nvss[:,1]):    \n",
    "        if mark < dec < mark + bin_size:\n",
    "            chunkis.append(i)\n",
    "            mark += bin_size\n",
    "\n",
    "    bins = [x/10 for x in range(int((nvss_dec_min-bin_size)*10),\n",
    "                                int((nvss_dec_max+bin_size+0.01)*10))]\n",
    "    cos_dec = np.array([np.cos(dec*np.pi/180) for dec in bins])\n",
    "\n",
    "    # useful lines to check bin creation successful\n",
    "    #print(chunkis[:5],'...',chunkis[-5:])\n",
    "    #print(bins[:5],'...',bins[-5:])\n",
    "\n",
    "    matches = []\n",
    "    bar = tqdm(total=len(tgss))\n",
    "\n",
    "    # warning: this can take at least 30 minutes to complete\n",
    "    for i1,p1 in enumerate(tgss):\n",
    "        # using the sorting of nvss\n",
    "        if p1[1] < nvss_dec_min - 0.1:\n",
    "            bar.update(1)\n",
    "            continue\n",
    "        elif p1[1] > nvss_dec_max + 0.1:\n",
    "            break\n",
    "\n",
    "        which_bin = bins.index(np.floor(p1[1]*10)/10)\n",
    "        # look both ways for which bin: -1, 0, +1\n",
    "        nslice = chunkis[which_bin-1],chunkis[which_bin+2]\n",
    "\n",
    "        for i2,p2 in enumerate(nvss[nslice[0]:nslice[1]]):\n",
    "            # careful with azimuth near poles\n",
    "            if (abs((p1[0]-p2[0])*cos_dec[which_bin]) < SEPARATION_DIST\n",
    "                    and abs(p1[1]-p2[1]) < SEPARATION_DIST):\n",
    "                matches.append((p1,p2))\n",
    "\n",
    "        bar.postfix = 'matches = {}'.format(len(matches))\n",
    "        bar.update(1)\n",
    "\n",
    "    # only crude matches, would have been nice to save the names with these\n",
    "    matches = np.array([(m[0][0],m[0][1],m[1][0],m[1][1]) for m in matches])\n",
    "    np.savetxt('sky_matches.csv', matches, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out/delete this (and only this) cell for alternative bypass\n",
    "# must have existing sky_matches.csv present in cwd for bypass along with surveys\n",
    "generate_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geodist(p1,p2):\n",
    "    \"\"\"given two points close on the unit sphere,\n",
    "    return their geodesic distance,\n",
    "    input points must be in radians!\"\"\"\n",
    "    ra1,dec1,ra2,dec2 = p1[0],p1[1],p2[0],p2[1]\n",
    "    # see https://en.wikipedia.org/wiki/Great-circle_distance#Formulae\n",
    "    decdiff = (dec1-dec2)/2\n",
    "    radiff  = (ra1-ra2)/2\n",
    "    better_circle = 2*np.arcsin(np.sqrt(np.sin(decdiff)**2\n",
    "                    + np.cos(dec1)*np.cos(dec2) * np.sin(radiff)**2))\n",
    "    r = 1\n",
    "    return better_circle*r\n",
    "\n",
    "def degdist(p1,p2):\n",
    "    \"\"\"calls geodist on argument points in degrees,\n",
    "    after converting from degrees to radians and back\"\"\"\n",
    "    return 180/np.pi*geodist([x*np.pi/180 for x in p1],\n",
    "                             [x*np.pi/180 for x in p2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's where the alternative bypass will pickup matches\n",
    "matches = np.loadtxt('sky_matches.csv',delimiter=',')\n",
    "matches = np.reshape(matches,(len(matches), 2, 2))\n",
    "# print('initial matches:',matches.shape)\n",
    "\n",
    "# filter with the proper geodesic distance metric\n",
    "# and begin constructing the catalogue\n",
    "# rows are to be: name_TGSS, ra_TGSS, dec_TGSS, name_NVSS, ra_NVSS, dec_NVSS,\n",
    "# separation, non_unique_flag, peak_flux_TGSS, peak_flux_NVSS, alpha\n",
    "catalogue = []\n",
    "for m in tqdm(matches):\n",
    "    # remember to maintain the established order of: tgss, nvss\n",
    "    p1,p2 = m\n",
    "    d = degdist(p1,p2)\n",
    "    if d < SEPARATION_DIST:\n",
    "        catalogue.append(('',p1[0],p1[1],\\\n",
    "                          '',p2[0],p2[1],\\\n",
    "                          d,0,0,0,0))\n",
    "catalogue = np.array(catalogue,dtype=object)\n",
    "# nice to know total number of matches\n",
    "print('catalogue:',catalogue.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag all non-one-to-one matches between TGSS and NVSS\n",
    "for i,m in enumerate(tqdm(catalogue)):\n",
    "    p1ra,p1dec,p2ra,p2dec = m[1],m[2],m[4],m[5]\n",
    "    # abusing sorting, check only those nearby for non-uniqueness\n",
    "    # nearby value is arbitrary, in testing nothing greater than 10 was needed\n",
    "    nearby = 10\n",
    "    if nearby < i < len(catalogue)-nearby:\n",
    "        rest_wo = np.concatenate((catalogue[i-nearby:i],catalogue[i+1:i+1+nearby]))\n",
    "    elif i < nearby:\n",
    "        rest_wo = np.concatenate((catalogue[:i],catalogue[i+1:i+1+nearby]))\n",
    "    elif len(catalogue)-nearby < i:\n",
    "        rest_wo = np.concatenate((catalogue[i-nearby:i],catalogue[i+1:]))\n",
    "\n",
    "    tgss_wo = rest_wo[:,(1,2)]\n",
    "    nvss_wo = rest_wo[:,(4,5)]\n",
    "    \n",
    "    if (np.any((tgss_wo[:]==[p1ra,p1dec]).all(1)) or\n",
    "        np.any((nvss_wo[:]==[p2ra,p2dec]).all(1))):\n",
    "        catalogue[i][7] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload surveys to recover names, fluxes, and calculate alpha\n",
    "tgss_labels = np.genfromtxt('TGSSADR1_7sigma_catalog.tsv',\n",
    "                            delimiter='\\t', skip_header=1, usecols=0, dtype=str)\n",
    "ftgss = np.genfromtxt('TGSSADR1_7sigma_catalog.tsv',\n",
    "                      delimiter='\\t', skip_header=1, usecols=(1,3,7))\n",
    "# unfortunate choice of variable name here, labels in the naming, not the ML sense\n",
    "tgss_labels = tgss_labels[ftgss[:,1].argsort()]\n",
    "ftgss = ftgss[ftgss[:,1].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(\"CATALOG.FIT\") as hdulist:\n",
    "    data = hdulist[1].data\n",
    "    # jansky?!/beam, is confusing units\n",
    "    # note that I don't correct for beamsize, this should have been done\n",
    "    fnvss = np.column_stack((data['RA(2000)'],data['DEC(2000)'],data['PEAK INT']))\n",
    "fnvss = fnvss[fnvss[:,1].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deci_deg_to_deg_min_sec(deci_deg):\n",
    "    \"\"\"converts degree measurement into degrees-minutes-seconds\n",
    "    this is a clean solution, credit goes to:\n",
    "    (https://stackoverflow.com/questions/2579535/\n",
    "    convert-dd-decimal-degrees-to-dms-degrees-minutes-seconds-in-python)\n",
    "    \"\"\"\n",
    "    is_positive = (deci_deg >= 0)\n",
    "    deci_deg = abs(deci_deg)\n",
    "    # divmod returns quotient and remainder\n",
    "    minutes,seconds = divmod(deci_deg*3600,60)\n",
    "    degrees,minutes = divmod(minutes,60)\n",
    "    degrees = degrees if is_positive else -degrees\n",
    "    return (degrees,minutes,seconds)\n",
    "\n",
    "def deci_deg_to_hr_min_sec(deci_deg):\n",
    "    \"\"\"converts degree measurement into hours-minutes-seconds\n",
    "    assumes that deci_deg is postitive\"\"\"\n",
    "    deci_hours = deci_deg/15.\n",
    "    schminutes,schmeconds = divmod(deci_hours*3600,60)\n",
    "    hours,schminutes = divmod(schminutes,60)   \n",
    "    return (hours,schminutes,schmeconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iau_designation(ra,dec):\n",
    "    \"\"\"generate NVSS names as per:\n",
    "    https://heasarc.gsfc.nasa.gov/W3Browse/all/nvss.html\n",
    "    There are four cases where there are pairs of sources which are\n",
    "    so close together that their names would be identical according\n",
    "    to this schema (see below), and the HEASARC has added suffixes\n",
    "    of 'a' (for the source with smaller RA) and 'b' (for the source\n",
    "    with the larger RA) in such cases in order to differentate them.\n",
    "    It was easier just to hard-code this in,\n",
    "    should really check if designation alreadys exists and compare\n",
    "    \"\"\"\n",
    "    hr,schmin,schmec = deci_deg_to_hr_min_sec(ra)\n",
    "    rhh = str(int(hr)).zfill(2)\n",
    "    rmm = str(int(schmin)).zfill(2)\n",
    "    rss = str(int(schmec - schmec%1)).zfill(2)\n",
    "\n",
    "    deg,minu,sec = deci_deg_to_deg_min_sec(dec)\n",
    "    sgn = '+' if deg>=0 else '-'\n",
    "    ddd = str(int(abs(deg))).zfill(2)\n",
    "    dmm = str(int(minu)).zfill(2)\n",
    "    dss = str(int(sec - sec%1)).zfill(2)\n",
    "\n",
    "    designation = ''.join(('NVSS J',rhh,rmm,rss,sgn,ddd,dmm,dss))\n",
    "    \n",
    "    close_pairs = {'NVSS J093731-102001':144.382,\n",
    "                   'NVSS J133156-121336':202.987,\n",
    "                   'NVSS J160612+000027':241.553,\n",
    "                   'NVSS J215552+380029':328.968}\n",
    "    if designation in close_pairs:\n",
    "        if ra < close_pairs[designation]:\n",
    "            designation = ''.join((designation,'a'))\n",
    "        else:\n",
    "            designation = ''.join((designation,'b'))         \n",
    "\n",
    "    return designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, an unfortunate variable name, these are names of sources, not ML labels\n",
    "nvss_labels = np.array([iau_designation(p[0],p[1]) for p in tqdm(fnvss)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgss in mJy/beam, beams in ''\n",
    "# here I commented out the beam correction, I cannot justify why\n",
    "# yet the spectral alpha comparison to the reference paper seems to hold\n",
    "tgssbeam = 25\n",
    "ftgss[:,2] = ftgss[:,2]*1e-3#*tgssbeam\n",
    "# nvss in Jy/beam\n",
    "nvssbeam = 45\n",
    "fnvss[:,2] = fnvss[:,2]#*nvssbeam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels, flux densities, and spectral index to catalogue\n",
    "# again, rows are: name_TGSS, ra_TGSS, dec_TGSS, name_NVSS, ra_NVSS, dec_NVSS,\n",
    "# separation, non_unique_flag, peak_flux_TGSS, peak_flux_NVSS, alpha\n",
    "FREQ_NVSS, FREQ_TGSS = 1.4e9,150e6\n",
    "\n",
    "for i,m in enumerate(tqdm(catalogue)):\n",
    "    tdec,ndec = m[2],m[5]\n",
    "    # searchsorted gives index value if equal to or next if not\n",
    "    ti = np.searchsorted(ftgss[:,1],tdec)\n",
    "    ni = np.searchsorted(fnvss[:,1],ndec)\n",
    "    t_name,n_name = tgss_labels[ti],nvss_labels[ni]\n",
    "    s_tgss,s_nvss = ftgss[ti][2],fnvss[ni][2]\n",
    "    alpha = np.log(s_tgss/s_nvss)/np.log(FREQ_NVSS/FREQ_TGSS)\n",
    "    \n",
    "    catalogue[i][0] = t_name\n",
    "    catalogue[i][3] = n_name\n",
    "    catalogue[i][8] = s_tgss\n",
    "    catalogue[i][9] = s_nvss\n",
    "    catalogue[i][10] = alpha\n",
    "\n",
    "catalogue_fmt = ('%s','%1.5f','%1.5f','%s','%1.14f','%1.14f',\n",
    "                 '%1.18f','%i','%1.5f','%1.5f','%1.5f')\n",
    "np.savetxt('sky_catalogue.csv', catalogue, delimiter=',',fmt = catalogue_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seps = [3600*val for val in catalogue[:,6]]\n",
    "\n",
    "# create histogram of angular separation of matches\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.hist(seps, bins=100,color = \"darkmagenta\", ec=\"orchid\")\n",
    "plt.xlabel(\"angular separation, '' (arcsec)\")\n",
    "plt.ylabel('counts')\n",
    "plt.title('distribution of angular separation of matches')\n",
    "plt.savefig('hist_angle.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram of alpha value of matches, to compare to the reference paper\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "alpha = catalogue[:,10]\n",
    "s_tgss = catalogue[:,8]*1e3\n",
    "# thresholds as in paper\n",
    "allalpha = alpha,alpha[s_tgss>50],alpha[s_tgss>100],alpha[s_tgss>150],alpha[s_tgss>200]\n",
    "\n",
    "fig,axis = plt.subplots(figsize=(14,7))\n",
    "plt.hist(allalpha, bins=200, histtype='step', stacked=False, fill=False,\\\n",
    "         label=['no cut','S_tgss>50mJy','S_tgss>100mJy','S_tgss>150mJy','S_tgss>200mJy'],\\\n",
    "         color=['black','limegreen','orange','magenta','darkblue'])\n",
    "plt.legend(loc='upper right')\n",
    "handles, labels = axis.get_legend_handles_labels()\n",
    "plt.legend(reversed(handles), reversed(labels))\n",
    "plt.xlim(0,2.5)\n",
    "plt.xlabel(\"observed spectral index\")\n",
    "plt.ylabel('counts')\n",
    "plt.title('distribution of observed spectral index of matches')\n",
    "plt.savefig('hist_alpha.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
