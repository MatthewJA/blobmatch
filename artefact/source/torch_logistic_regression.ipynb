{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"torch_logistic_regression.ipynb\n",
    "James Gardner 2019\n",
    "with help from Matthew Alger wrt pytorch\n",
    "\n",
    "performs logistic regression on feature vectors\n",
    "against positional matching labels using pytorch\n",
    "\n",
    "required patch_catalogue.csv be present in cwd\n",
    "as output by feature_vectors.ipynb as well as manual_labels.csv\n",
    "\n",
    "will save the following:\n",
    "weights.csv, predictions.csv, objects.csv, multi_objects.csv,\n",
    "torch_lr_losses.pdf, torch_lr_weights.pdf, torch_lr_predictions.pdf, torch_lr_partition.pdf\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the patch catalogue (load the dataset and make it iterable)\n",
    "catalogue = pd.read_csv('patch_catalogue.csv')\n",
    "catalogue.set_index(['name_TGSS','name_NVSS'],inplace=True)\n",
    "\n",
    "scores = catalogue['score']\n",
    "# remove positions, could test to see if it recovers separation?\n",
    "del (catalogue['ra_TGSS'],catalogue['dec_TGSS'],\n",
    "     catalogue['ra_NVSS'],catalogue['dec_NVSS'],\n",
    "     catalogue['score'])\n",
    "\n",
    "# these derived log features prove more useful than the regular values\n",
    "catalogue['log_flux_TGSS']       = np.log10(catalogue['peak_TGSS'])\n",
    "catalogue['log_integrated_TGSS'] = np.log10(catalogue['integrated_TGSS'])\n",
    "catalogue['log_ratio_flux_TGSS'] = np.log10(catalogue['peak_TGSS']/\n",
    "                                            catalogue['integrated_TGSS'])\n",
    "catalogue['log_flux_NVSS']       = np.log10(catalogue['peak_NVSS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features and labels within pytorch\n",
    "# scores are out of separation scorer, so 0.1 should likely be 0\n",
    "labels = (scores.values > 0.1)\n",
    "features = catalogue.values\n",
    "\n",
    "# train on half the catalogue (A), predict against the whole thing (A+B)\n",
    "labels_A = labels[::2]\n",
    "# labels_B = labels[1::2]\n",
    "features_A = features[::2]\n",
    "# features_B = features[1::2]\n",
    "\n",
    "labels_A = Variable(torch.from_numpy(labels_A).float())\n",
    "# labels_B = Variable(torch.from_numpy(labels_B).float())\n",
    "features_A = Variable(torch.Tensor(features_A))\n",
    "# features_B = Variable(torch.Tensor(features_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model class\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # for our uses, the output layer is binary classification\n",
    "        self.linear = torch.nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # and here's the sigmoid!\n",
    "        outputs = F.sigmoid(self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some of the training hyper-parameters\n",
    "input_dim = features_A.shape[1]\n",
    "# learning rate cf. time-step in physical simulations\n",
    "learning_rate = 0.001\n",
    "# an epoch is a total cycle through all the training data\n",
    "# increase this value if the losses plot doesn't appear to stabilise\n",
    "num_epochs = int(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model, criterion (i.e. loss), and optimizer classes\n",
    "model = LogisticRegression(input_dim)\n",
    "# binary cross entropy, standard use\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "# stochastic gradient decent cf. unbiased estimate of a noisy observation\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model, this can take some time depending on num_epochs\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # reset gradient accumulation\n",
    "    optimizer.zero_grad()\n",
    "    # forward step: predict and find loss\n",
    "    predictions = model(features_A)\n",
    "    loss = criterion(predictions, labels_A)\n",
    "    # use .item() to stop memory leak to GPU, advice from M.Alger\n",
    "    losses.append(loss.item())\n",
    "    # backwards step: use loss to optimize a little bit\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram of the loss trend, hopefully shows some stabilisation\n",
    "# if it doesn't, try increasing num_epochs\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.plot(losses)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('torch_lr - losses showing stabilisation')\n",
    "plt.savefig('torch_lr_losses.pdf',bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights and bias to reconstruct model if needs be\n",
    "parameters = list(model.parameters())\n",
    "weights = parameters[0].detach().numpy().ravel()\n",
    "bias = parameters[1].detach().numpy()\n",
    "\n",
    "np.savetxt('weights.csv', np.concatenate((weights,bias)), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot of classifier weights\n",
    "# of particular interest: separation, alpha, log_flux_NVSS\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.bar(range(len(weights)),weights)\n",
    "plt.xlabel('weights')\n",
    "plt.xticks(range(len(weights)),catalogue.columns,rotation='vertical')\n",
    "plt.ylabel('co-eff')\n",
    "plt.title('torch_lr - weights')\n",
    "plt.savefig('torch_lr_weights.pdf',bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the entire catalogue and compare to labels\n",
    "features_cat = Variable(torch.Tensor(features))\n",
    "predictions_cat = model(features_cat).detach().numpy()\n",
    "\n",
    "# where the two populations cross is where we say the classifier decides the split\n",
    "nc_100 = np.histogram(predictions_cat[labels == 0],bins=np.arange(0,1,0.01),density=True)[0]\n",
    "pc_100 = np.histogram(predictions_cat[labels == 1],bins=np.arange(0,1,0.01),density=True)[0]\n",
    "midpoint = 0.5\n",
    "for i in range(len(nc_100)):\n",
    "    if nc_100[i] < pc_100[i]:\n",
    "        midpoint = 0.01*i\n",
    "        break\n",
    "\n",
    "pred_labels_cat = (predictions_cat > midpoint).astype(float)\n",
    "pred_labels_cat = np.array([x[0] for x in pred_labels_cat])\n",
    "\n",
    "# unconditional, note the native 63% negative class bias\n",
    "accuracy = (pred_labels_cat == labels).mean()\n",
    "# precision (true if said so)\n",
    "precision = (labels[pred_labels_cat == True] == True).mean()\n",
    "# recall (said so if true)\n",
    "recall    = (pred_labels_cat[labels == True] == True).mean()\n",
    "print(('over whole catalogue:\\n accuracy = {0:.3f}, precision = {1:.3f}, recall = {2:.3f}')\n",
    "      .format(accuracy,precision,recall))\n",
    "\n",
    "# saves names of match and predicted label\n",
    "catalogue['pred_labels'] = pred_labels_cat\n",
    "catalogue['pred_labels'].to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sigmoid(y):\n",
    "    \"\"\"given: y = 1/(e^-x+1)\"\"\"\n",
    "    x = np.log(y/(1-y))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram of predictions with populations separated off of label\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,figsize=(14,16))\n",
    "ax1.set_title('logistic regression predictions \\n \\n score, h(x)')\n",
    "ax2.set_title('class probability, g(x)')\n",
    "ax3.set_title('class prediction, f(x)')\n",
    "ax1.set_ylabel('pdf')\n",
    "ax2.set_ylabel('pdf')\n",
    "ax1.hist((inv_sigmoid(predictions_cat[labels == 0]),\n",
    "          inv_sigmoid(predictions_cat[labels == 1])), bins=100,\n",
    "         histtype='step', label=('negative class','postive class'), color = ('red','blue'), density = True)\n",
    "ax1.legend()\n",
    "ax2.hist((predictions_cat[labels == 0],predictions_cat[labels == 1]), bins=100,\n",
    "         histtype='step', label=('negative class','postive class'), color = ('red','blue'), density = True)\n",
    "ax2.legend()\n",
    "\n",
    "negative_class = np.histogram(predictions_cat[labels == 0],bins=[0,midpoint,1],density=True)[0]\n",
    "positive_class = np.histogram(predictions_cat[labels == 1],bins=[0,midpoint,1],density=True)[0]\n",
    "\n",
    "ax3.bar(np.array((0,0.75)),negative_class,0.2, label='negative class', edgecolor='red', color='None')\n",
    "ax3.bar(np.array((0.25,1)),positive_class,0.2, label='positive class', edgecolor='blue', color='None')\n",
    "ax3.set_xticks((0,0.25,0.75,1))\n",
    "ax3.set_xticklabels(('0','0','1','1'))\n",
    "ax3.set_ylabel('pdf, binned as [0,{},1]'.format(midpoint))\n",
    "ax3.legend()\n",
    "\n",
    "plt.savefig('torch_lr_predictions.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy against manual labels\n",
    "manual_labels = pd.read_csv('manual_labels.csv')\n",
    "manual_labels.set_index(['name_TGSS','name_NVSS'],inplace=True)\n",
    "man_cat = catalogue.loc[manual_labels.index.values]\n",
    "\n",
    "label_man = manual_labels['manual_label'].values\n",
    "pred_man = man_cat['pred_labels']\n",
    "\n",
    "accuracy = (pred_man == label_man).mean()\n",
    "precision = (label_man[pred_man == True] == True).mean()\n",
    "recall    = (pred_man[label_man == True] == True).mean() \n",
    "print(('on manual labels:\\n accuracy = {0:.3f}, precision = {1:.3f}, recall = {2:.3f}')\n",
    "      .format(accuracy,precision,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the sky into physical objects using classifier\n",
    "# we do this naively, by transitively linking together matches\n",
    "# critical is that this naive partitioning can be bad given a good classifier\n",
    "cat_pairs = set(catalogue.index.values)\n",
    "obj_pairs = []\n",
    "\n",
    "for pair in tqdm(cat_pairs):\n",
    "    if catalogue.loc[pair]['pred_labels'] == 1:\n",
    "        obj_pairs.append(pair)\n",
    "\n",
    "objects = {}\n",
    "tnames = {}\n",
    "nnames = {}\n",
    "\n",
    "index = 0\n",
    "for pair in tqdm(obj_pairs):\n",
    "    tname, nname = pair[0], pair[1]\n",
    "    \n",
    "    if not tname in tnames and not nname in nnames:\n",
    "        i = index\n",
    "        objects[i] = [tname,nname]\n",
    "        tnames[tname] = i\n",
    "        nnames[nname] = i\n",
    "    elif tname in tnames and not nname in nnames:\n",
    "        i = tnames[tname]\n",
    "        objects[i].append(nname)\n",
    "        nnames[nname] = i\n",
    "    elif not tname in tnames and nname in nnames:\n",
    "        i = nnames[nname]\n",
    "        objects[i].append(tname)\n",
    "        tnames[tname] = i\n",
    "    elif tname in tnames and nname in nnames:\n",
    "        # must merge objects, zig-zag problem\n",
    "        i = tnames[tname]\n",
    "        j = nnames[nname]\n",
    "        if i == j:\n",
    "            continue\n",
    "        else:\n",
    "            obj_i = objects[i]\n",
    "            obj_j = objects[j]\n",
    "            merged_obj = list(set(obj_i+obj_j))\n",
    "            objects[index] = merged_obj\n",
    "            del objects[i], objects[j] \n",
    "            for name in merged_obj:\n",
    "                if   name[0] == 'T':\n",
    "                    tnames[name] = index\n",
    "                elif name[0] == 'N':\n",
    "                    nnames[name] = index\n",
    "        \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most interesting objects, those with many components\n",
    "multi_objects = {}\n",
    "most_components = 0\n",
    "most_components_i = 0\n",
    "for key, val in objects.items():\n",
    "    if len(val) > 2:\n",
    "        multi_objects[key] = val\n",
    "        if len(val) > most_components:\n",
    "            most_components = len(val)\n",
    "            most_components_i = key\n",
    "# the extreme amount of components here is a sign that the naive partioning is indeed naive\n",
    "print(most_components, multi_objects[most_components_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save object partition\n",
    "def dict_to_csv(dict_to_convert, filename):\n",
    "    values = []\n",
    "    for val in dict_to_convert.values():\n",
    "        values.append(val)\n",
    "\n",
    "    with open(filename, 'w', newline = '') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(values)\n",
    "    \n",
    "dict_to_csv(objects,'objects.csv')\n",
    "dict_to_csv(multi_objects,'multi_objects.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_the_dots(centre,field_of_view):\n",
    "    \"\"\"creates a picture of the object partition in the square\n",
    "    about the centre (tuple of ra, dec in degrees) of fov (degrees)\n",
    "    only shows the links between sources across surveys, i.e. 'matches'\n",
    "    \"\"\"\n",
    "    c_ra, c_dec = centre\n",
    "    w_fov = field_of_view\n",
    "    \n",
    "    lookup_cat = pd.read_csv('patch_catalogue.csv', usecols=['name_TGSS','name_NVSS',\n",
    "                                                            'ra_TGSS','dec_TGSS','ra_NVSS','dec_NVSS'])\n",
    "    lookup_cat.set_index(['name_TGSS','name_NVSS'],inplace=True)\n",
    "    lookup_cat['pred_labels'] = pred_labels_cat\n",
    "\n",
    "    # find all objects/links within a 3 degree window of centre\n",
    "    window = lookup_cat[(lookup_cat['pred_labels']==1) &\n",
    "                        (lookup_cat['ra_TGSS']>c_ra-w_fov) &\n",
    "                        (lookup_cat['ra_TGSS']<c_ra+w_fov) &\n",
    "                        (lookup_cat['dec_TGSS']>c_dec-w_fov) &\n",
    "                        (lookup_cat['dec_TGSS']<c_dec+w_fov) &\n",
    "                        (lookup_cat['ra_NVSS']>c_ra-w_fov) &\n",
    "                        (lookup_cat['ra_NVSS']<c_ra+w_fov) &\n",
    "                        (lookup_cat['dec_NVSS']>c_dec-w_fov) &\n",
    "                        (lookup_cat['dec_NVSS']<c_dec+w_fov)]\n",
    "\n",
    "    del window['pred_labels'], lookup_cat\n",
    "    walues = window.values\n",
    "    del window    \n",
    "    tgss_x = np.reshape(walues[:,[0]],len(walues))\n",
    "    tgss_y = np.reshape(walues[:,[1]],len(walues))\n",
    "    nvss_x = np.reshape(walues[:,[2]],len(walues))\n",
    "    nvss_y = np.reshape(walues[:,[3]],len(walues))\n",
    "    \n",
    "    plt.figure(figsize=(14,14))\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.plot(tgss_x,tgss_y,'r,')\n",
    "    plt.plot(nvss_x,nvss_y,'b,')\n",
    "\n",
    "    for i in tqdm(range(len(walues))):\n",
    "        plt.plot([tgss_x[i],nvss_x[i]],[tgss_y[i],nvss_y[i]],'k-',linewidth=0.5)\n",
    "\n",
    "    # invert x-axis to read as RA from right to left\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ax.set_xlim(xlim[::-1])\n",
    "        \n",
    "    plt.title('Naive partition of TGSS to NVSS in sky around {0:.2f},{1:.2f}'.format(c_ra,c_dec))\n",
    "    plt.ylabel('DEC / °')\n",
    "    plt.xlabel('RA / °')\n",
    "    plt.savefig('torch_lr_partition.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting centre candidates:\n",
    "# 153.65,-27.09 | J101436.8-270532, for the many-component object about the centre\n",
    "# 166.10,-27.16, 158.60 -15.58, 152.64,-18.01\n",
    "centre = 153.65,-27.09\n",
    "field_of_view = 5\n",
    "connect_the_dots(centre,field_of_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
